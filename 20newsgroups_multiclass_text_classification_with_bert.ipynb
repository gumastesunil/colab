{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " 20newsgroups multiclass  text classification with bert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gumastesunil/colab/blob/master/20newsgroups_multiclass_text_classification_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYVJkGin7h2i",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2019 Google Inc.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUXBo_RIHqc0",
        "colab_type": "text"
      },
      "source": [
        "Predicting newggroup classes with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp-DX1YpHdCX",
        "colab_type": "text"
      },
      "source": [
        "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gmLCboN-WuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnd5BfWmH5uM",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMKsTQ1JWCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set logging level\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELQaoTa0-9Xy",
        "colab_type": "code",
        "outputId": "f789b3c4-4f6f-41d3-a344-53b2f00f2d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNWanpVd_B6p",
        "colab_type": "code",
        "outputId": "4114db3b-41a3-405d-c4d2-9e0565c1ed24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 13:50:13.651510 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbN7dyMAmdLT",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lip08xmp_HuT",
        "colab_type": "code",
        "outputId": "601cf932-0c4d-4bfb-a433-e7a378c2d250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: OUTPUT_DIR_NAME *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-6GzeTdmkdI",
        "colab_type": "text"
      },
      "source": [
        "Get dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-bFYL3rwCdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4cebaa70-d244-4ae7-8169-2df5da2561f6"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "print('Keys from train:{}'.format(newsgroups_train.keys()))\n",
        "print('Keys from test :{}'.format(newsgroups_test.keys()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "I0630 13:50:27.083852 139702197761920 twenty_newsgroups.py:247] Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "I0630 13:50:27.092934 139702197761920 twenty_newsgroups.py:80] Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Keys from train:dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
            "Keys from test :dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3RrTtxrmyyA",
        "colab_type": "text"
      },
      "source": [
        "Get the data and labels into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJxM88CoF7kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame(columns=['message','class'], data=zip(newsgroups_train.data,newsgroups_train.target))\n",
        "test = pd.DataFrame(columns=['message','class'], data=zip(newsgroups_test.data,newsgroups_test.target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eYLd_llm2qi",
        "colab_type": "text"
      },
      "source": [
        "Retain only alphanumeric characters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHPIW9p5E0ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['message'] = train['message'].str.replace('[^0-9a-zA-Z.]', ' ')  # Retain a-z,A-Z,0-9 and '.'(full stop)\n",
        "test['message'] = test['message'].str.replace('[^0-9a-zA-Z.]', ' ')    # Retain a-z,A-Z,0-9 and '.'(full stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVAcOP62jkbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f6c2b553-1ddc-418e-febb-db756150bf80"
      },
      "source": [
        "le = LabelEncoder()\n",
        "enc_labels = le.fit_transform(newsgroups_train.target_names)\n",
        "print('Target labels : {}'.format(newsgroups_train.target_names))\n",
        "print('Encoded labels : {}'.format(enc_labels))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target labels : ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Encoded labels : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MIHlIwEnnOx",
        "colab_type": "text"
      },
      "source": [
        "For us, our input data is the 'message' column and our label is the 'class' column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiDU23PE_uAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'message'\n",
        "LABEL_COLUMN = 'class'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = enc_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMiIqLuKnzEY",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq3k4wHV_3Dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySWe8dYn5QA",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FTLwPTKn75R",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UExoqoM_5nJ",
        "colab_type": "code",
        "outputId": "70e12a30-666a-4ffc-ff4d-646252a0514e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 13:51:52.334678 139702197761920 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0630 13:51:56.411402 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1wlPRBKoBtY",
        "colab_type": "text"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMI3BB7S_-WB",
        "colab_type": "code",
        "outputId": "1b75c81a-14a1-49ec-fb77-7f55af3184bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddvg9H5ToGAD",
        "colab_type": "text"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiiNJYEeADRx",
        "colab_type": "code",
        "outputId": "8bb69404-753e-437f-84f3-f2d7e66174bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 13:52:10.593682 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0630 13:52:10.598215 139702197761920 run_classifier.py:774] Writing example 0 of 11314\n",
            "I0630 13:52:10.611178 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:52:10.615344 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:52:10.617996 139702197761920 run_classifier.py:464] tokens: [CLS] i was wondering if anyone out there could en ##light ##en me on this car i saw the other day . it was a 2 door sports car looked to be from the late 60s early 70s . it was called a brick ##lin . the doors were really small . in addition the front bumper was separate from the rest of the body . this is all i know . if anyone can tell ##me a model name engine spec ##s years of production where this car is made history or whatever info you have on this funky looking car please e mail . [SEP]\n",
            "I0630 13:52:10.619721 139702197761920 run_classifier.py:465] input_ids: 101 1045 2001 6603 2065 3087 2041 2045 2071 4372 7138 2368 2033 2006 2023 2482 1045 2387 1996 2060 2154 1012 2009 2001 1037 1016 2341 2998 2482 2246 2000 2022 2013 1996 2397 20341 2220 17549 1012 2009 2001 2170 1037 5318 4115 1012 1996 4303 2020 2428 2235 1012 1999 2804 1996 2392 21519 2001 3584 2013 1996 2717 1997 1996 2303 1012 2023 2003 2035 1045 2113 1012 2065 3087 2064 2425 4168 1037 2944 2171 3194 28699 2015 2086 1997 2537 2073 2023 2482 2003 2081 2381 2030 3649 18558 2017 2031 2006 2023 24151 2559 2482 3531 1041 5653 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.620912 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.622319 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.624022 139702197761920 run_classifier.py:468] label: 7 (id = 7)\n",
            "I0630 13:52:10.630508 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:52:10.632601 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:52:10.634572 139702197761920 run_classifier.py:464] tokens: [CLS] a fair number of brave souls who upgraded their si clock os ##ci ##lla ##tor have shared their experiences for this poll . please send a brief message detailing your experiences with the procedure . top speed attained cpu rated speed add on cards and adapt ##ers heat sinks hour of usage per day floppy disk functionality with 800 and 1 . 4 m flop ##pies are especially requested . i will be sum ##mar ##izing in the next two days so please add to the network knowledge base if you have done the clock upgrade and haven t answered this poll . thanks . [SEP]\n",
            "I0630 13:52:10.636923 139702197761920 run_classifier.py:465] input_ids: 101 1037 4189 2193 1997 9191 9293 2040 9725 2037 9033 5119 9808 6895 4571 4263 2031 4207 2037 6322 2005 2023 8554 1012 3531 4604 1037 4766 4471 17555 2115 6322 2007 1996 7709 1012 2327 3177 12754 17368 6758 3177 5587 2006 5329 1998 15581 2545 3684 23462 3178 1997 8192 2566 2154 28491 9785 15380 2007 5385 1998 1015 1012 1018 1049 28583 13046 2024 2926 7303 1012 1045 2097 2022 7680 7849 6026 1999 1996 2279 2048 2420 2061 3531 5587 2000 1996 2897 3716 2918 2065 2017 2031 2589 1996 5119 12200 1998 4033 1056 4660 2023 8554 1012 4283 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.638794 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.640753 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.642537 139702197761920 run_classifier.py:468] label: 4 (id = 4)\n",
            "I0630 13:52:10.659009 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:52:10.661167 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:52:10.663416 139702197761920 run_classifier.py:464] tokens: [CLS] well folks my mac plus finally gave up the ghost this weekend after starting life as a 512 ##k way back in 1985 . soo ##o i m in the market for a new machine a bit sooner than i intended to be . . . i m looking into picking up a power ##book 160 or maybe 180 and have a bunch of questions that hopefully somebody can answer does anybody know any dirt on when the next round of power ##book introductions are expected i d heard the 185 ##c was supposed to make an appear ##ence this summer but haven t heard anymore on it and since i don t have access to mac ##lea ##k i was wondering if anybody out there [SEP]\n",
            "I0630 13:52:10.665273 139702197761920 run_classifier.py:465] input_ids: 101 2092 12455 2026 6097 4606 2633 2435 2039 1996 5745 2023 5353 2044 3225 2166 2004 1037 24406 2243 2126 2067 1999 3106 1012 17111 2080 1045 1049 1999 1996 3006 2005 1037 2047 3698 1037 2978 10076 2084 1045 3832 2000 2022 1012 1012 1012 1045 1049 2559 2046 8130 2039 1037 2373 8654 8148 2030 2672 8380 1998 2031 1037 9129 1997 3980 2008 11504 8307 2064 3437 2515 10334 2113 2151 6900 2006 2043 1996 2279 2461 1997 2373 8654 25795 2024 3517 1045 1040 2657 1996 15376 2278 2001 4011 2000 2191 2019 3711 10127 2023 2621 2021 4033 1056 2657 4902 2006 2009 1998 2144 1045 2123 1056 2031 3229 2000 6097 19738 2243 1045 2001 6603 2065 10334 2041 2045 102\n",
            "I0630 13:52:10.667842 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 13:52:10.670271 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.672170 139702197761920 run_classifier.py:468] label: 4 (id = 4)\n",
            "I0630 13:52:10.675022 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:52:10.676845 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:52:10.678572 139702197761920 run_classifier.py:464] tokens: [CLS] do you have wei ##tek s address phone number i d like to get some information about this chip . [SEP]\n",
            "I0630 13:52:10.680484 139702197761920 run_classifier.py:465] input_ids: 101 2079 2017 2031 11417 23125 1055 4769 3042 2193 1045 1040 2066 2000 2131 2070 2592 2055 2023 9090 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.682261 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.684386 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.687093 139702197761920 run_classifier.py:468] label: 1 (id = 1)\n",
            "I0630 13:52:10.700954 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:52:10.702789 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:52:10.704746 139702197761920 run_classifier.py:464] tokens: [CLS] from article c ##5 ##ow ##cb . n ##3 ##p world . st ##d . com by tomb ##aker world . st ##d . com tom a baker my understanding is that the expected errors are basically known bugs in the warning system software things are checked that don t have the right values in yet because they aren t set till after launch and such ##like . rather than fix the code and possibly introduce new bugs they just tell the crew ok if you see a warning no . 213 before lift ##off ignore it . [SEP]\n",
            "I0630 13:52:10.706585 139702197761920 run_classifier.py:465] input_ids: 101 2013 3720 1039 2629 5004 27421 1012 1050 2509 2361 2088 1012 2358 2094 1012 4012 2011 8136 22626 2088 1012 2358 2094 1012 4012 3419 1037 6243 2026 4824 2003 2008 1996 3517 10697 2024 10468 2124 12883 1999 1996 5432 2291 4007 2477 2024 7039 2008 2123 1056 2031 1996 2157 5300 1999 2664 2138 2027 4995 1056 2275 6229 2044 4888 1998 2107 10359 1012 2738 2084 8081 1996 3642 1998 4298 8970 2047 12883 2027 2074 2425 1996 3626 7929 2065 2017 2156 1037 5432 2053 1012 19883 2077 6336 7245 8568 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.708913 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.710740 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:52:10.712603 139702197761920 run_classifier.py:468] label: 14 (id = 14)\n",
            "I0630 13:52:56.165967 139702197761920 run_classifier.py:774] Writing example 10000 of 11314\n",
            "I0630 13:53:01.456079 139702197761920 run_classifier.py:774] Writing example 0 of 7532\n",
            "I0630 13:53:01.459476 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:53:01.460654 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:53:01.469555 139702197761920 run_classifier.py:464] tokens: [CLS] i am a little confused on all of the models of the 88 89 bonn ##eville ##s . i have heard of the le se l ##se ss ##e ss ##ei . could someone tell me the differences are far as features or performance . i am also curious to know what the book value is for prefer ##ea ##bly the 89 model . and how much less than book value can you usually get them for . in other words how much are they in demand this time of year . i have heard that the mid spring early summer is the best time to buy . [SEP]\n",
            "I0630 13:53:01.475351 139702197761920 run_classifier.py:465] input_ids: 101 1045 2572 1037 2210 5457 2006 2035 1997 1996 4275 1997 1996 6070 6486 19349 21187 2015 1012 1045 2031 2657 1997 1996 3393 7367 1048 3366 7020 2063 7020 7416 1012 2071 2619 2425 2033 1996 5966 2024 2521 2004 2838 2030 2836 1012 1045 2572 2036 8025 2000 2113 2054 1996 2338 3643 2003 2005 9544 5243 6321 1996 6486 2944 1012 1998 2129 2172 2625 2084 2338 3643 2064 2017 2788 2131 2068 2005 1012 1999 2060 2616 2129 2172 2024 2027 1999 5157 2023 2051 1997 2095 1012 1045 2031 2657 2008 1996 3054 3500 2220 2621 2003 1996 2190 2051 2000 4965 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.476898 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.478889 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.481199 139702197761920 run_classifier.py:468] label: 7 (id = 7)\n",
            "I0630 13:53:01.491692 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:53:01.493082 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:53:01.494728 139702197761920 run_classifier.py:464] tokens: [CLS] i m not familiar at all with the format of these x face thing ##ies but after seeing them in some folks header ##s i ve got to see them and maybe make one of my own i ve got d ##pg view on my linux box which displays un ##com ##pressed x faces and i ve managed to com ##pile un com ##pf ##ace too . . . but now that i m looking for them i can t seem to find any x face s in anyone ##s news header ##s could you would you please send me your x face header i know i ll probably get a little swamp ##ed but i can handle it . . . . i hope . [SEP]\n",
            "I0630 13:53:01.496562 139702197761920 run_classifier.py:465] input_ids: 101 1045 1049 2025 5220 2012 2035 2007 1996 4289 1997 2122 1060 2227 2518 3111 2021 2044 3773 2068 1999 2070 12455 20346 2015 1045 2310 2288 2000 2156 2068 1998 2672 2191 2028 1997 2026 2219 1045 2310 2288 1040 26952 3193 2006 2026 11603 3482 2029 8834 4895 9006 19811 1060 5344 1998 1045 2310 3266 2000 4012 22090 4895 4012 14376 10732 2205 1012 1012 1012 2021 2085 2008 1045 1049 2559 2005 2068 1045 2064 1056 4025 2000 2424 2151 1060 2227 1055 1999 3087 2015 2739 20346 2015 2071 2017 2052 2017 3531 4604 2033 2115 1060 2227 20346 1045 2113 1045 2222 2763 2131 1037 2210 11963 2098 2021 1045 2064 5047 2009 1012 1012 1012 1012 1045 3246 1012 102\n",
            "I0630 13:53:01.499545 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 13:53:01.502672 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.504770 139702197761920 run_classifier.py:468] label: 5 (id = 5)\n",
            "I0630 13:53:01.506917 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:53:01.509214 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:53:01.511069 139702197761920 run_classifier.py:464] tokens: [CLS] in a word yes . [SEP]\n",
            "I0630 13:53:01.513463 139702197761920 run_classifier.py:465] input_ids: 101 1999 1037 2773 2748 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.515417 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.517962 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.521309 139702197761920 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 13:53:01.556085 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:53:01.557374 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:53:01.558514 139702197761920 run_classifier.py:464] tokens: [CLS] they were attacking the iraqi ##s to drive them out of kuwait a country whose citizens have close blood and business ties to saudi citizens . and me thinks if the us had not helped out the iraqi ##s would have swallowed saudi arabia too or at least the eastern oil ##fields . and no muslim country was doing much of anything to help li ##ber ##ate kuwait and protect saudi arabia indeed in some masses of citizens were demonstrating in favor of that butcher saddam who killed lots ##a muslims just because he was killing rap ##ing and looting relatively rich muslims and also thumb ##ing his nose at the west . so how would have you defended saudi arabia and rolled back the iraqi [SEP]\n",
            "I0630 13:53:01.560123 139702197761920 run_classifier.py:465] input_ids: 101 2027 2020 7866 1996 8956 2015 2000 3298 2068 2041 1997 13085 1037 2406 3005 4480 2031 2485 2668 1998 2449 7208 2000 8174 4480 1012 1998 2033 6732 2065 1996 2149 2018 2025 3271 2041 1996 8956 2015 2052 2031 7351 8174 9264 2205 2030 2012 2560 1996 2789 3514 15155 1012 1998 2053 5152 2406 2001 2725 2172 1997 2505 2000 2393 5622 5677 3686 13085 1998 4047 8174 9264 5262 1999 2070 11678 1997 4480 2020 14313 1999 5684 1997 2008 14998 24111 2040 2730 7167 2050 7486 2074 2138 2002 2001 4288 9680 2075 1998 29367 4659 4138 7486 1998 2036 7639 2075 2010 4451 2012 1996 2225 1012 2061 2129 2052 2031 2017 8047 8174 9264 1998 4565 2067 1996 8956 102\n",
            "I0630 13:53:01.561838 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 13:53:01.563680 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.565993 139702197761920 run_classifier.py:468] label: 17 (id = 17)\n",
            "I0630 13:53:01.573798 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 13:53:01.577624 139702197761920 run_classifier.py:462] guid: None\n",
            "I0630 13:53:01.579763 139702197761920 run_classifier.py:464] tokens: [CLS] i ve just spent two solid months arguing that no such thing as an objective moral system exists . [SEP]\n",
            "I0630 13:53:01.581637 139702197761920 run_classifier.py:465] input_ids: 101 1045 2310 2074 2985 2048 5024 2706 9177 2008 2053 2107 2518 2004 2019 7863 7191 2291 6526 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.583810 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.586171 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 13:53:01.588119 139702197761920 run_classifier.py:468] label: 19 (id = 19)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7lq1hbnoQ-K",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwwBO0d5AH_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "                  BERT_MODEL_HUB,\n",
        "                  trainable=True)\n",
        "  bert_inputs = dict(\n",
        "                  input_ids=input_ids,\n",
        "                  input_mask=input_mask,\n",
        "                  segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "                  inputs=bert_inputs,\n",
        "                  signature=\"tokens\",\n",
        "                  as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-t1vFmoVDI",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce9RRDDKAU0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        '''\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        '''        \n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy\n",
        "            # \"f1_score\": f1_score,\n",
        "            # \"auc\": auc,\n",
        "            # \"precision\": precision,\n",
        "            # \"recall\": recall,\n",
        "            # \"true_positives\": true_pos,\n",
        "            # \"true_negatives\": true_neg,\n",
        "            # \"false_positives\": false_pos,\n",
        "            # \"false_negatives\": false_neg                        \n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcHrg6JPAY3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c6NSHpgAcXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-KiWCq4AfkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKT0fQFrAiyh",
        "colab_type": "code",
        "outputId": "539582bd-013a-4898-c765-5e3687d6b2d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 13:54:20.175814 139702197761920 estimator.py:209] Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0e51db2048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmStHDPiodP6",
        "colab_type": "text"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3KodAArAl5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpfEpqwgoipf",
        "colab_type": "text"
      },
      "source": [
        "Now we train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr9r3RsGAsja",
        "colab_type": "code",
        "outputId": "4c14f9e6-56cd-4804-de65-deb7c3a2f481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 13:54:32.140211 139702197761920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0630 13:54:38.666912 139702197761920 estimator.py:1145] Calling model_fn.\n",
            "I0630 13:54:42.540642 139702197761920 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0630 13:54:42.744880 139702197761920 deprecation.py:506] From <ipython-input-15-3a9a6318aaf0>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0630 13:54:42.793988 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0630 13:54:42.797037 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0630 13:54:42.808530 139702197761920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0630 13:54:42.827723 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0630 13:54:43.142564 139702197761920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0630 13:54:47.856292 139702197761920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0630 13:54:53.637077 139702197761920 estimator.py:1147] Done calling model_fn.\n",
            "I0630 13:54:53.640632 139702197761920 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0630 13:54:57.996625 139702197761920 monitored_session.py:240] Graph was finalized.\n",
            "I0630 13:55:04.353047 139702197761920 session_manager.py:500] Running local_init_op.\n",
            "I0630 13:55:04.595413 139702197761920 session_manager.py:502] Done running local_init_op.\n",
            "I0630 13:55:17.709557 139702197761920 basic_session_run_hooks.py:606] Saving checkpoints for 0 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "I0630 13:55:37.824281 139702197761920 basic_session_run_hooks.py:262] loss = 3.0295048, step = 0\n",
            "I0630 13:58:27.575337 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.589095\n",
            "I0630 13:58:27.577013 139702197761920 basic_session_run_hooks.py:260] loss = 1.5776435, step = 100 (169.753 sec)\n",
            "I0630 14:00:58.016626 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.664711\n",
            "I0630 14:00:58.018530 139702197761920 basic_session_run_hooks.py:260] loss = 1.3086966, step = 200 (150.442 sec)\n",
            "I0630 14:03:27.900524 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.667183\n",
            "I0630 14:03:27.902175 139702197761920 basic_session_run_hooks.py:260] loss = 1.074975, step = 300 (149.884 sec)\n",
            "I0630 14:05:58.441306 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.664272\n",
            "I0630 14:05:58.443277 139702197761920 basic_session_run_hooks.py:260] loss = 0.753582, step = 400 (150.541 sec)\n",
            "I0630 14:08:27.462230 139702197761920 basic_session_run_hooks.py:606] Saving checkpoints for 500 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "I0630 14:08:38.220040 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.625865\n",
            "I0630 14:08:38.221615 139702197761920 basic_session_run_hooks.py:260] loss = 0.60371196, step = 500 (159.778 sec)\n",
            "I0630 14:11:08.596633 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.664997\n",
            "I0630 14:11:08.599204 139702197761920 basic_session_run_hooks.py:260] loss = 0.624142, step = 600 (150.378 sec)\n",
            "I0630 14:13:39.313695 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.663495\n",
            "I0630 14:13:39.315538 139702197761920 basic_session_run_hooks.py:260] loss = 0.46946758, step = 700 (150.716 sec)\n",
            "I0630 14:16:10.993467 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.659284\n",
            "I0630 14:16:10.995777 139702197761920 basic_session_run_hooks.py:260] loss = 0.6168637, step = 800 (151.680 sec)\n",
            "I0630 14:18:42.227465 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.661227\n",
            "I0630 14:18:42.229253 139702197761920 basic_session_run_hooks.py:260] loss = 0.13920857, step = 900 (151.233 sec)\n",
            "I0630 14:21:11.205915 139702197761920 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "I0630 14:21:22.008006 139702197761920 basic_session_run_hooks.py:692] global_step/sec: 0.625858\n",
            "I0630 14:21:22.009854 139702197761920 basic_session_run_hooks.py:260] loss = 0.42484877, step = 1000 (159.781 sec)\n",
            "I0630 14:22:50.883803 139702197761920 basic_session_run_hooks.py:606] Saving checkpoints for 1060 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "I0630 14:23:00.764984 139702197761920 estimator.py:368] Loss for final step: 0.2928589.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:28:28.643163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDIHOl0_ooGw",
        "colab_type": "text"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K__uULIvEFlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gly7JiZ1EHPu",
        "colab_type": "code",
        "outputId": "976e0062-9402-46f7-83d9-e0e7c03b4f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 14:46:59.828508 139702197761920 estimator.py:1145] Calling model_fn.\n",
            "I0630 14:47:04.258002 139702197761920 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0630 14:47:15.438067 139702197761920 estimator.py:1147] Done calling model_fn.\n",
            "I0630 14:47:15.469949 139702197761920 evaluation.py:255] Starting evaluation at 2019-06-30T14:47:15Z\n",
            "I0630 14:47:17.587858 139702197761920 monitored_session.py:240] Graph was finalized.\n",
            "W0630 14:47:17.601617 139702197761920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0630 14:47:17.610715 139702197761920 saver.py:1280] Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1060\n",
            "I0630 14:47:20.691894 139702197761920 session_manager.py:500] Running local_init_op.\n",
            "I0630 14:47:20.969215 139702197761920 session_manager.py:502] Done running local_init_op.\n",
            "I0630 14:49:20.642581 139702197761920 evaluation.py:275] Finished evaluation at 2019-06-30-14:49:20\n",
            "I0630 14:49:20.644432 139702197761920 estimator.py:2039] Saving dict for global step 1060: eval_accuracy = 0.7057886, global_step = 1060, loss = 1.0185767\n",
            "I0630 14:49:23.438384 139702197761920 estimator.py:2099] Saving 'checkpoint_path' summary for global step 1060: OUTPUT_DIR_NAME/model.ckpt-1060\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.7057886, 'global_step': 1060, 'loss': 1.0185767}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypy0bon2o3xP",
        "colab_type": "text"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtuponYiEp9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = le.inverse_transform(enc_labels)\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  # return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n",
        "  return [labels[prediction['labels']] for prediction in predictions]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z0bWFdGEt93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]\n",
        "'''\n",
        "pred_sentences = [\n",
        "    newsgroups_test.data[10],\n",
        "    newsgroups_test.data[20],\n",
        "    newsgroups_test.data[30],\n",
        "    newsgroups_test.data[40] \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqXi_B7YEycG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "6f2a37a8-4d6a-4ad0-9091-6579ce97cc09"
      },
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0630 14:49:43.919896 139702197761920 run_classifier.py:774] Writing example 0 of 4\n",
            "I0630 14:49:43.924296 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 14:49:43.930670 139702197761920 run_classifier.py:462] guid: \n",
            "I0630 14:49:43.936773 139702197761920 run_classifier.py:464] tokens: [CLS] i have uploaded the windows on - line review share ##ware edition to ft ##p . ci ##ca . indiana . ed ##u as / pub / pc / win ##3 / up ##load ##s / wo ##lr ##s ##7 . zip . it is an on - line magazine which contains reviews of some share ##ware products . . . i grabbed it from the windows on - line bb ##s . - - [SEP]\n",
            "I0630 14:49:43.938469 139702197761920 run_classifier.py:465] input_ids: 101 1045 2031 21345 1996 3645 2006 1011 2240 3319 3745 8059 3179 2000 3027 2361 1012 25022 3540 1012 5242 1012 3968 2226 2004 1013 9047 1013 7473 1013 2663 2509 1013 2039 11066 2015 1013 24185 20974 2015 2581 1012 14101 1012 2009 2003 2019 2006 1011 2240 2932 2029 3397 4391 1997 2070 3745 8059 3688 1012 1012 1012 1045 4046 2009 2013 1996 3645 2006 1011 2240 22861 2015 1012 1011 1011 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:43.941089 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:43.942699 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:43.944288 139702197761920 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 14:49:43.974663 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 14:49:43.975938 139702197761920 run_classifier.py:462] guid: \n",
            "I0630 14:49:43.977585 139702197761920 run_classifier.py:464] tokens: [CLS] this is an invitation to send articles to the inform ##atic ##a magazine . the first fully international issue has been published and echoes are quite favourable . for any information , contact ( mat ##ja ##z . ga ##ms @ i ##js . si ) . dear colleague , april 25 , 1993 number 1 of volume 17 of inform ##atic ##a is now out of print and some of you will receive it in a week or so . as you will see , the journal is structured in the following way : the editorial ( first page ) ; profiles ( second page - - biography of an editor , in this issue , terry win ##og ##rad ) ; the edited part [SEP]\n",
            "I0630 14:49:43.978919 139702197761920 run_classifier.py:465] input_ids: 101 2023 2003 2019 8468 2000 4604 4790 2000 1996 12367 12070 2050 2932 1012 1996 2034 3929 2248 3277 2038 2042 2405 1998 17659 2024 3243 18731 1012 2005 2151 2592 1010 3967 1006 13523 3900 2480 1012 11721 5244 1030 1045 22578 1012 9033 1007 1012 6203 11729 1010 2258 2423 1010 2857 2193 1015 1997 3872 2459 1997 12367 12070 2050 2003 2085 2041 1997 6140 1998 2070 1997 2017 2097 4374 2009 1999 1037 2733 2030 2061 1012 2004 2017 2097 2156 1010 1996 3485 2003 14336 1999 1996 2206 2126 1024 1996 8368 1006 2034 3931 1007 1025 17879 1006 2117 3931 1011 1011 8308 1997 2019 3559 1010 1999 2023 3277 1010 6609 2663 8649 12173 1007 1025 1996 5493 2112 102\n",
            "I0630 14:49:43.980419 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 14:49:43.982018 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:43.985375 139702197761920 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 14:49:43.994638 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 14:49:43.996608 139702197761920 run_classifier.py:462] guid: \n",
            "I0630 14:49:43.998062 139702197761920 run_classifier.py:464] tokens: [CLS] i am writing a x - based dose ##mu which requires x ##key ##rel ##eased ##eve ##nt . i found the key ##code of x ##key ##rel ##eased ##eve ##nt is wrong . if i run the program on a linux host ( x ##free ##1 . 2 ) with display set to the local linux and to the sun host ( x ##11 ##r ##5 ) , the two key ##codes from the two x ##ser ##vers are different . of course , the key ##code of x ##key ##pressed ##eve ##nt is o . k . can anybody verify this ? did i do anything wrong ? thanks . [SEP]\n",
            "I0630 14:49:43.999681 139702197761920 run_classifier.py:465] input_ids: 101 1045 2572 3015 1037 1060 1011 2241 13004 12274 2029 5942 1060 14839 16570 25063 18697 3372 1012 1045 2179 1996 3145 16044 1997 1060 14839 16570 25063 18697 3372 2003 3308 1012 2065 1045 2448 1996 2565 2006 1037 11603 3677 1006 1060 23301 2487 1012 1016 1007 2007 4653 2275 2000 1996 2334 11603 1998 2000 1996 3103 3677 1006 1060 14526 2099 2629 1007 1010 1996 2048 3145 23237 2013 1996 2048 1060 8043 14028 2024 2367 1012 1997 2607 1010 1996 3145 16044 1997 1060 14839 19811 18697 3372 2003 1051 1012 1047 1012 2064 10334 20410 2023 1029 2106 1045 2079 2505 3308 1029 4283 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:44.002273 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:44.006376 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:44.010774 139702197761920 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 14:49:44.036792 139702197761920 run_classifier.py:461] *** Example ***\n",
            "I0630 14:49:44.038295 139702197761920 run_classifier.py:462] guid: \n",
            "I0630 14:49:44.039853 139702197761920 run_classifier.py:464] tokens: [CLS] er ##gin ##el ) asked : [ ee ] no , no flaming here . just a simple question . . . . with a simple answer ! [ ee ] as far as i know most of the armenians belong to the gregorian orthodox [ ee ] faith and such was the case in nineteenth century ottoman empire . it is [ ee ] also known that some armenian communities were converted into catholicism [ ee ] and protestant ##ism by the western european missionaries in this period . the vast majority of armenians in eastern anatolia were gregorian or armenian apostolic . there was , however , a higher percentage of non - gregorian armenians in ci ##lic ##ia , closer to the mediterranean [SEP]\n",
            "I0630 14:49:44.042584 139702197761920 run_classifier.py:465] input_ids: 101 9413 11528 2884 1007 2356 1024 1031 25212 1033 2053 1010 2053 19091 2182 1012 2074 1037 3722 3160 1012 1012 1012 1012 2007 1037 3722 3437 999 1031 25212 1033 2004 2521 2004 1045 2113 2087 1997 1996 20337 7141 2000 1996 25847 6244 1031 25212 1033 4752 1998 2107 2001 1996 2553 1999 9137 2301 6188 3400 1012 2009 2003 1031 25212 1033 2036 2124 2008 2070 7508 4279 2020 4991 2046 16138 1031 25212 1033 1998 8330 2964 2011 1996 2530 2647 11743 1999 2023 2558 1012 1996 6565 3484 1997 20337 1999 2789 23747 2020 25847 2030 7508 11815 1012 2045 2001 1010 2174 1010 1037 3020 7017 1997 2512 1011 25847 20337 1999 25022 10415 2401 1010 3553 2000 1996 7095 102\n",
            "I0630 14:49:44.045467 139702197761920 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0630 14:49:44.047502 139702197761920 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0630 14:49:44.050378 139702197761920 run_classifier.py:468] label: 0 (id = 0)\n",
            "I0630 14:49:44.090989 139702197761920 estimator.py:1145] Calling model_fn.\n",
            "I0630 14:49:48.213067 139702197761920 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0630 14:49:48.454948 139702197761920 estimator.py:1147] Done calling model_fn.\n",
            "I0630 14:49:49.040191 139702197761920 monitored_session.py:240] Graph was finalized.\n",
            "I0630 14:49:49.054590 139702197761920 saver.py:1280] Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1060\n",
            "I0630 14:49:50.107315 139702197761920 session_manager.py:500] Running local_init_op.\n",
            "I0630 14:49:50.229691 139702197761920 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gjnUdwFE2DA",
        "colab_type": "code",
        "outputId": "3557aab7-5407-442c-b226-2718bc319fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['comp.os.ms-windows.misc',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.windows.x',\n",
              " 'talk.politics.mideast']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}